{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19o65K4DiQ_k",
        "outputId": "fc4fa2f5-9649-43e8-9916-242011841404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "1ME_Bj8KHnLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install tensorflow-federated"
      ],
      "metadata": {
        "id": "3rd-jTKQpmkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYrq4I03ydy8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization,GRU, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.keras.metrics import Recall\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import precision_score, recall_score,accuracy_score\n",
        "from tensorflow.keras.initializers import HeUniform\n",
        "from tensorflow.keras import regularizers\n",
        "import os\n",
        "import sklearn.metrics\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import keras as ks\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "#import tensorflow_federated as tff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.enable_eager_execution()"
      ],
      "metadata": {
        "id": "J8fyt5NC2j0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX9kBCvZ4NbU"
      },
      "outputs": [],
      "source": [
        "dataset1 = pd.read_csv('/content/drive/MyDrive/4144.csv', index_col='ts', parse_dates=True)\n",
        "dataset2 = pd.read_csv('/content/drive/MyDrive/4147.csv', index_col='ts', parse_dates=True)\n",
        "dataset3 = pd.read_csv('/content/drive/MyDrive/4135.csv', index_col='ts', parse_dates=True)\n",
        "dataset4 = pd.read_csv('/content/drive/MyDrive/4137.csv', index_col='ts', parse_dates=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1"
      ],
      "metadata": {
        "id": "E0DEiJJnZ4XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs =30\n",
        "num_rounds=4\n",
        "\n",
        "timesteps = 100\n"
      ],
      "metadata": {
        "id": "0bncJdxvWGy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddt1wTTt62w0"
      },
      "outputs": [],
      "source": [
        "def split_data_chronologically(data, test_ratio=0.2, val_ratio=0.2, target_column='anomaly_label'):\n",
        "    total_samples = len(data)\n",
        "    test_split_idx = int(total_samples * (1 - test_ratio))\n",
        "    val_split_idx = int(test_split_idx * (1 - val_ratio / (1 - test_ratio)))\n",
        "\n",
        "    train_data = data.iloc[:val_split_idx]\n",
        "    val_data = data.iloc[val_split_idx:test_split_idx]\n",
        "    test_data = data.iloc[test_split_idx:]\n",
        "\n",
        "    train_labels = train_data[target_column].values.astype('float32')\n",
        "    val_labels = val_data[target_column].values.astype('float32')\n",
        "    test_labels = test_data[target_column].values.astype('float32')\n",
        "\n",
        "    train_data = train_data.drop(columns=[target_column]).astype('float32')\n",
        "    test_data = test_data.drop(columns=[target_column]).astype('float32')\n",
        "    val_data = val_data.drop(columns=[target_column]).astype('float32')\n",
        "\n",
        "    return train_data, val_data, test_data, train_labels, test_labels, val_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCIeJer7LMCo"
      },
      "outputs": [],
      "source": [
        "def create_tf_dataset(features, labels, time_steps=timesteps, batch_size=32, shuffle=False):\n",
        "    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "        data=features,\n",
        "        targets=labels,\n",
        "        sequence_length=time_steps,\n",
        "        sequence_stride=1,\n",
        "        shuffle=shuffle,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(data, labels, time_steps=timesteps, batch_size=128):\n",
        "    # Cast data and labels to float32\n",
        "    data = tf.cast(data, tf.float32)\n",
        "    labels = tf.cast(labels, tf.float32)\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
        "    dataset = dataset.window(time_steps, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(time_steps), y.batch(time_steps))))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset.repeat(1)#.batch(batch_size).map(batch_format_fn).prefeth(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "GXmF_6hgNihl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      ks.metrics.TruePositives(name='tp'),\n",
        "      ks.metrics.FalsePositives(name='fp'),\n",
        "      ks.metrics.TrueNegatives(name='tn'),\n",
        "      ks.metrics.FalseNegatives(name='fn'),\n",
        "      ks.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      ks.metrics.Precision(name='precision'),\n",
        "      ks.metrics.Recall(name='recall'),\n",
        "      ks.metrics.AUC(name='auc'),\n",
        "      ks.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]"
      ],
      "metadata": {
        "id": "Clej0T4i8eqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_data(dataset):\n",
        "    # Split data chronologically\n",
        "    train_data, val_data, test_data, train_labels, test_labels, val_labels = split_data_chronologically(dataset)\n",
        "\n",
        "    # Convert data to numpy arrays\n",
        "    train_data = train_data.to_numpy().astype('float32')\n",
        "    val_data = val_data.to_numpy().astype('float32')\n",
        "    test_data = test_data.to_numpy().astype('float32')\n",
        "\n",
        "    # Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_data)\n",
        "    train_data = scaler.transform(train_data)\n",
        "    test_data = scaler.transform(test_data)\n",
        "    val_data = scaler.transform(val_data)\n",
        "\n",
        "    return train_data, val_data, test_data, train_labels, test_labels, val_labels\n"
      ],
      "metadata": {
        "id": "gWTFWWYP9De8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess2(data, labels, time_steps=timesteps, batch_size=128):\n",
        "     dataset= create_tf_dataset(data,labels, time_steps=time_steps, batch_size=batch_size)\n",
        "\n",
        "     return dataset.repeat(num_epochs)"
      ],
      "metadata": {
        "id": "wR3M4kndGIw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data1, val_data1, test_data1, train_labels1, test_labels1, val_labels1 = setup_data(dataset1)\n",
        "train_data2, val_data2, test_data2, train_labels2, test_labels2, val_labels2 = setup_data(dataset2)\n",
        "train_data3, val_data3, test_data3, train_labels3, test_labels3, val_labels3 = setup_data(dataset3)\n",
        "train_data4, val_data4, test_data4, train_labels4, test_labels4, val_labels4 = setup_data(dataset4)"
      ],
      "metadata": {
        "id": "xTxA5rYZI2qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_features  = train_data1.shape[-1]"
      ],
      "metadata": {
        "id": "Y_jRT6cxannq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client_ids = ['client1', 'client2', 'client3', 'client4']\n"
      ],
      "metadata": {
        "id": "pUztUDBdlT-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_datasets =   [train_data1, train_data2, train_data3, train_data4]\n",
        "train_labels = [train_labels1, train_labels2, train_labels3, train_labels4]\n",
        "val_datasets=[val_data1, val_data2, val_data3, val_data4]\n",
        "val_labels=[val_labels1, val_labels2, val_labels3, val_labels4]\n",
        "test_datasets=[test_data1, test_data2, test_data3, test_data4]\n",
        "test_labels =[test_labels1, test_labels2, test_labels3, test_labels4]"
      ],
      "metadata": {
        "id": "GfvZx-xLKwrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyClientData(tff.simulation.datasets.ClientData):\n",
        "\n",
        "    def __init__(self, client_ids, datasets):\n",
        "        self._client_ids = client_ids\n",
        "        self._datasets = datasets\n",
        "\n",
        "    @property\n",
        "    def client_ids(self):\n",
        "        return self._client_ids\n",
        "\n",
        "    def create_tf_dataset_for_client(self, client_id):\n",
        "        return self._datasets[self._client_ids.index(client_id)]\n",
        "\n",
        "    @property\n",
        "    def element_type_structure(self):\n",
        "        return (tf.TensorSpec(shape=(None, 100, 14), dtype=tf.float32), tf.TensorSpec(shape=(None, 100), dtype=tf.float32))\n",
        "\n",
        "    def serializable_dataset_fn(self):\n",
        "        return tf.function(lambda client_id: self.create_tf_dataset_for_client(client_id), input_signature=[tf.TensorSpec(shape=(), dtype=tf.string)])\n",
        "\n"
      ],
      "metadata": {
        "id": "vL8OgogymZ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets = [preprocess2(train_datasets[i], train_labels[i]) for i in range(len(client_ids))]\n",
        "val_datasets = [preprocess2(val_datasets[i], val_labels[i]) for i in range(len(client_ids))]\n",
        "test_datasets = [preprocess2(test_datasets[i], test_labels[i]) for i in range(len(client_ids))]"
      ],
      "metadata": {
        "id": "Pic2-8DKoxHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create your datasets\n",
        "\n",
        "\n",
        "# Create ClientData objects\n",
        "train_client_data = MyClientData(client_ids, train_datasets)\n",
        "val_client_data = MyClientData(client_ids, val_datasets)\n",
        "test_client_data = MyClientData(client_ids, test_datasets)\n",
        "\n",
        "# Create lists of datasets\n",
        "federated_train_data = [train_client_data.create_tf_dataset_for_client(client_id) for client_id in client_ids]\n",
        "federated_val_data = [val_client_data.create_tf_dataset_for_client(client_id) for client_id in client_ids]\n",
        "federated_test_data = [test_client_data.create_tf_dataset_for_client(client_id) for client_id in client_ids]\n",
        "\n"
      ],
      "metadata": {
        "id": "QygDCWQOk-LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "federated_test_data[0].element_spec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xLDxfw6wR7Q",
        "outputId": "7bbda8bd-5c5e-4239-c072-1ca41146c53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(None, None, 13), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "   input_shape = (timesteps, num_features)\n",
        "   model = tf.keras.models.Sequential([\n",
        "            tf.keras.layers.Input(shape=input_shape),\n",
        "            tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "            tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
        "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "            tf.keras.layers.LSTM(128, activation='relu'),  # Optional dense layer after flattening\n",
        "            tf.keras.layers.Dropout(0.5),  # Optional dropout layer\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "   return model\n",
        "\n"
      ],
      "metadata": {
        "id": "JWjB9lIi-jMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "B_JRCj9KyLSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f061a7d5-32f6-455f-cc8d-3af360664b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 100, 32)           1280      \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 50, 32)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 50, 32)            128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 50, 32)            0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 50, 64)            6208      \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 25, 64)            0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 25, 64)            256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 25, 64)            0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 25, 128)           24704     \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 12, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 12, 128)           512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 128)           0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 164801 (643.75 KB)\n",
            "Trainable params: 164353 (642.00 KB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def model_fn():\n",
        "    keras_model = create_model()\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=federated_test_data[0].element_spec,\n",
        "        loss=tf.keras.losses.BinaryFocalCrossentropy(alpha=0.7, gamma=4),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "tklYwPYYYpY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def client_optimizer_fn():\n",
        "    return tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "def server_optimizer_fn():\n",
        "    return tf.keras.optimizers.Adam(learning_rate=0.1)"
      ],
      "metadata": {
        "id": "ygSHz5heDKIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=client_optimizer_fn,\n",
        "    server_optimizer_fn=server_optimizer_fn\n",
        ")\n",
        "\n",
        "evaluation_process = tff.learning.algorithms.build_fed_eval(model_fn)"
      ],
      "metadata": {
        "id": "r8-u79uaFi4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_sizing_environment():\n",
        "  \"\"\"Creates an environment that contains sizing information.\"\"\"\n",
        "  # Creates a sizing executor factory to output communication cost\n",
        "  # after the training finishes. Note that sizing executor only provides an\n",
        "  # estimate (not exact) of communication cost, and doesn't capture cases like\n",
        "  # compression of over-the-wire representations. However, it's perfect for\n",
        "  # demonstrating the effect of compression in this tutorial.\n",
        "  sizing_factory = tff.framework.ExecutorFactory.create_executor()\n",
        "\n",
        "  # TFF has a modular runtime you can configure yourself for various\n",
        "  # environments and purposes, and this example just shows how to configure one\n",
        "  # part of it to report the size of things.\n",
        "  context = tff.framework.ExecutionContext(executor_fn=sizing_factory)\n",
        "  tff.framework.set_default_context(context)\n",
        "\n",
        "  return sizing_factory"
      ],
      "metadata": {
        "id": "FEBTWsyDEGHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_size(size):\n",
        "  \"\"\"A helper function for creating a human-readable size.\"\"\"\n",
        "  size = float(size)\n",
        "  for unit in ['B','KiB','MiB','GiB']:\n",
        "    if size < 1024.0:\n",
        "      return \"{size:3.2f}{unit}\".format(size=size, unit=unit)\n",
        "    size /= 1024.0\n",
        "  return \"{size:.2f}{unit}\".format(size=size, unit='TiB')"
      ],
      "metadata": {
        "id": "LMxYjz37GhXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_process.initialize.type_signature.formatted_representation())"
      ],
      "metadata": {
        "id": "OAMRl5IPwqoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Train Model\n",
        "def train_federated_model(federated_train_data, federated_val_data, federated_test_data):\n",
        "\n",
        "    #environment = set_sizing_environment()\n",
        "\n",
        "    train_state = training_process.initialize()\n",
        "    evaluation_state = evaluation_process.initialize()\n",
        "    model_weights = training_process.get_model_weights(train_state)\n",
        "    evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
        "\n",
        "    NUM_ROUNDS = 2\n",
        "    for round_num in range(1, NUM_ROUNDS):\n",
        "            result = training_process.next(train_state, federated_train_data)\n",
        "            train_state = result.state\n",
        "            train_metrics = result.metrics\n",
        "            #size_info = environment.get_size_info()\n",
        "            #broadcasted_bits = size_info.broadcast_bits[-1]\n",
        "            #aggregated_bits = size_info.aggregate_bits[-1]\n",
        "            print('round {:2d}, metrics={}'.format(round_num, train_metrics))# format_size(broadcasted_bits), format_size(aggregated_bits)))\n",
        "            model_weights = training_process.get_model_weights(train_state)\n",
        "            evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
        "            evaluation_output = evaluation_process.next(evaluation_state, federated_val_data)\n",
        "            val_metrics = evaluation_output.metrics\n",
        "            print('Round {:2d}, Validation Metrics: {}'.format(round_num, val_metrics))\n",
        "\n",
        "\n",
        "    evaluation_test = evaluation_process.next(evaluation_state, federated_test_data)\n",
        "    test_metrics = evaluation_test.metrics\n",
        "    print('Round {:2d}, Test Metrics: {}'.format(NUM_ROUNDS, test_metrics))\n",
        "\n"
      ],
      "metadata": {
        "id": "C1w1CyowVwA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -R /content/drive/MyDrive/logfed/*\n",
        "\n",
        "log_dir = '/content/drive/MyDrive/logfed/original/'\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)"
      ],
      "metadata": {
        "id": "Fo2fMqf9JVD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Train Model\n",
        "train_federated_model(federated_train_data, federated_val_data, federated_test_data)"
      ],
      "metadata": {
        "id": "fn9c8n6oW4k3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9829478-0917-47aa-b118-9b15fe7bdc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round  1, metrics=OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.96581334), ('precision', 0.9445769), ('recall', 0.92066574), ('loss', 0.024789102), ('num_examples', 49335150), ('num_batches', 385500)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round  1, Validation Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('eval', OrderedDict([('current_round_metrics', OrderedDict([('binary_accuracy', 0.76909727), ('precision', 0.16666667), ('recall', 7.904576e-06), ('loss', 0.15645836), ('num_examples', 16437180), ('num_batches', 128460)])), ('total_rounds_metrics', OrderedDict([('binary_accuracy', 0.76909727), ('precision', 0.16666667), ('recall', 7.904576e-06), ('loss', 0.15645836), ('num_examples', 16437180), ('num_batches', 128460)]))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n",
            "Round  2, Test Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('eval', OrderedDict([('current_round_metrics', OrderedDict([('binary_accuracy', 0.74269354), ('precision', 0.0), ('recall', 0.0), ('loss', 0.15584189), ('num_examples', 16437210), ('num_batches', 128460)])), ('total_rounds_metrics', OrderedDict([('binary_accuracy', 0.74269354), ('precision', 0.0), ('recall', 0.0), ('loss', 0.15584189), ('num_examples', 16437210), ('num_batches', 128460)]))]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', ())])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_ROUNDS = 3\n",
        "for round_num in range(1, NUM_ROUNDS):\n",
        "  result = training_process.next(train_state, federated_train_data)\n",
        "  train_state = result.state\n",
        "  train_metrics = result.metrics\n",
        "  print('round {:2d}, metrics={}'.format(round_num, train_metrics))\n",
        "  model_weights = training_process.get_model_weights(train_state)\n",
        "  evaluation_state = evaluation_process.set_model_weights(evaluation_state, model_weights)\n",
        "  evaluation_output = evaluation_process.next(evaluation_state, federated_val_data)\n",
        "  val_metrics = evaluation_output.metrics\n",
        "  print('Round {:2d}, Validation Metrics: {}'.format(round_num, val_metrics))\n",
        "\n",
        "evaluation_test = evaluation_process.next(evaluation_state, federated_test_data)\n",
        "test_metrics = evaluation_test.metrics\n",
        "print(test_metrics={test_metrics})"
      ],
      "metadata": {
        "id": "oxhVRY1ew5v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_output = evaluation_process.next(evaluation_state, federated_train_data)"
      ],
      "metadata": {
        "id": "GijIBh9EzVtv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}